{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Градиентный бустинг своими руками\n",
        "\n",
        "**Внимание:** в тексте задания произошли изменения - поменялось число деревьев (теперь 50), правило изменения величины шага в задании 3 и добавился параметр `random_state` у решающего дерева. Правильные ответы не поменялись, но теперь их проще получить. Также исправлена опечатка в функции `gbm_predict`.\n",
        "\n",
        "В этом задании будет использоваться датасет `boston` из `sklearn.datasets`. Оставьте последние 25% объектов для контроля качества, разделив `X` и `y` на `X_train`, `y_train` и `X_test`, `y_test`.\n",
        "\n",
        "Целью задания будет реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: отключение предупреждений\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: подготовка данных"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "from sklearn import datasets, model_selection, tree\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#vibo: загружаем датасет\n",
        "boston = datasets.load_boston()\n",
        "\n",
        "#vibo: ключи датасета\n",
        "boston.keys()"
      ],
      "execution_count":20,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: датасет о стоимости жилья в Бостоне\n",
        "print(boston.DESCR)"
      ],
      "execution_count":21,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric\/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "#vibo: выделяем подвыборки для обучения и контроля качества (на тест последние 25% по улсловию задачи)\n",
        "train_size = 0.75\n",
        "train_size_number = int(y.shape[0] * train_size)\n",
        "X_train = X[:train_size_number]\n",
        "X_test = X[train_size_number:]\n",
        "y_train = y[:train_size_number]\n",
        "y_test = y[train_size_number:]"
      ],
      "execution_count":22,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Задание 1\n",
        "\n",
        "Как вы уже знаете из лекций, **бустинг** - это метод построения композиций базовых алгоритмов с помощью последовательного добавления к текущей композиции нового алгоритма с некоторым коэффициентом. \n",
        "\n",
        "Градиентный бустинг обучает каждый новый алгоритм так, чтобы он приближал антиградиент ошибки по ответам композиции на обучающей выборке. Аналогично минимизации функций методом градиентного спуска, в градиентном бустинге мы подправляем композицию, изменяя алгоритм в направлении антиградиента ошибки.\n",
        "\n",
        "Воспользуйтесь формулой из лекций, задающей ответы на обучающей выборке, на которые нужно обучать новый алгоритм (фактически это лишь чуть более подробно расписанный градиент от ошибки), и получите частный ее случай, если функция потерь `L` - квадрат отклонения ответа композиции `a(x)` от правильного ответа `y` на данном `x`.\n",
        "\n",
        "Если вы давно не считали производную самостоятельно, вам поможет таблица производных элементарных функций (которую несложно найти в интернете) и правило дифференцирования сложной функции. После дифференцирования квадрата у вас возникнет множитель 2 — т.к. нам все равно предстоит выбирать коэффициент, с которым будет добавлен новый базовый алгоритм, проигноируйте этот множитель при дальнейшем построении алгоритма."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Задание 2\n",
        "\n",
        "Заведите массив для объектов `DecisionTreeRegressor` (будем их использовать в качестве базовых алгоритмов) и для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами). \n",
        "\n",
        "В цикле от обучите последовательно 50 решающих деревьев с параметрами `max_depth=5` и `random_state=42` (остальные параметры - по умолчанию). В бустинге зачастую используются сотни и тысячи деревьев, но мы ограничимся 50, чтобы алгоритм работал быстрее, и его было проще отлаживать (т.к. цель задания разобраться, как работает метод). Каждое дерево должно обучаться на одном и том же множестве объектов, но ответы, которые учится прогнозировать дерево, будут меняться в соответствие с полученным в задании 1 правилом. \n",
        "\n",
        "Попробуйте для начала всегда брать коэффициент равным 0.9. Обычно оправдано выбирать коэффициент значительно меньшим - порядка 0.05 или 0.1, но т.к. в нашем учебном примере на стандартном датасете будет всего 50 деревьев, возьмем для начала шаг побольше.\n",
        "\n",
        "В процессе реализации обучения вам потребуется функция, которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке `X`:\n",
        "\n",
        "```\n",
        "def gbm_predict(X):\n",
        "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
        "(считаем, что base_algorithms_list - список с базовыми алгоритмами, coefficients_list - список с коэффициентами перед алгоритмами)\n",
        "```\n",
        "\n",
        "Эта же функция поможет вам получить прогноз на контрольной выборке и оценить качество работы вашего алгоритма с помощью `mean_squared_error` в `sklearn.metrics`. \n",
        "\n",
        "Возведите результат в степень 0.5, чтобы получить `RMSE`. Полученное значение `RMSE` — **ответ в пункте 2**."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: Решения Задания 2. Цель задания - реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь (величина шага постоянная 0.9)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем число решающих деревьев (базовых алгоритмов)\n",
        "N = 50\n",
        "\n",
        "#vibo: список с базовыми алгоритмами\n",
        "base_algorithms_list = []\n",
        "\n",
        "#vibo: список с коэффициентами перед алгоритмами\n",
        "coefficients_list = []\n",
        "\n",
        "#vibo: шаг алгоритма\n",
        "eta = 0.9\n",
        "\n",
        "#vibo: список ошибок\n",
        "mse_root_list = []\n",
        "\n",
        "#vibo: функция которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке X\n",
        "def gbm_predict(X):\n",
        "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]"
      ],
      "execution_count":23,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем начальный алгоритм и обучаем его\n",
        "regressor = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "#vibo: обучаем на train\n",
        "regressor.fit(X_train, y_train)\n",
        "#vibo: добавляем первым в список\n",
        "base_algorithms_list.append(regressor)\n",
        "#vibo: добавляем первый коэффициент в список\n",
        "coefficients_list.append(eta)\n",
        "\n",
        "#vibo: обучаем решающие деревья\n",
        "for i in range(N-1):\n",
        "    #vibo: рассчитываем сдвиг на train как истинный ответ минус предсказание\n",
        "    #vibo: при работе функции gmb_predict берем из списка алгоритмов - начальный, считаем прогноз, домножаем на eta\n",
        "    s = y_train - gbm_predict(X_train)\n",
        "    #vibo: задаем алгоритм\n",
        "    regressor = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "    #vibo: обучаем алгоритм, приближаем к s\n",
        "    regressor.fit(X_train, s)\n",
        "    base_algorithms_list.append(regressor)\n",
        "    coefficients_list.append(eta)\n",
        "    #vibo: вычисляем ошибку на test\n",
        "    mse_root = (mean_squared_error(y_test, gbm_predict(X_test)))**0.5 \n",
        "    mse_root_list.append(mse_root)"
      ],
      "execution_count":24,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mse_root_list"
      ],
      "execution_count":5,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "[4.671153714252575,\n",
              " 4.902307279003964,\n",
              " 4.964147618734462,\n",
              " 5.20526450047539,\n",
              " 5.135565602127196,\n",
              " 5.240746955112687,\n",
              " 5.289270262665005,\n",
              " 5.474743913112658,\n",
              " 5.487351059181247,\n",
              " 5.493005009464344,\n",
              " 5.486541185059441,\n",
              " 5.485319403332965,\n",
              " 5.491142418867942,\n",
              " 5.4234289382521625,\n",
              " 5.455189053902552,\n",
              " 5.453903636946008,\n",
              " 5.442535087357115,\n",
              " 5.451577011033647,\n",
              " 5.450957836386989,\n",
              " 5.455454044759808,\n",
              " 5.452830834182104,\n",
              " 5.4534823020413565,\n",
              " 5.456955189013637,\n",
              " 5.4585969199592395,\n",
              " 5.4597046298350955,\n",
              " 5.458441954443328,\n",
              " 5.459544166421452,\n",
              " 5.474640780590228,\n",
              " 5.455978129276078,\n",
              " 5.456647037889836,\n",
              " 5.456688163206112,\n",
              " 5.455089494285212,\n",
              " 5.454920050184027,\n",
              " 5.454737355535651,\n",
              " 5.4557529752777665,\n",
              " 5.457222277871692,\n",
              " 5.456585785858681,\n",
              " 5.454982869324787,\n",
              " 5.455803988401056,\n",
              " 5.455699568702136,\n",
              " 5.455320529878214,\n",
              " 5.454721673397888,\n",
              " 5.454973956294515,\n",
              " 5.455111351969862,\n",
              " 5.455731058970443,\n",
              " 5.4557186100299795,\n",
              " 5.455788482463016,\n",
              " 5.455738872267899,\n",
              " 5.455623403859612]"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def write_answer(answer, filename):\n",
        "    with open(filename, 'w') as fout:\n",
        "        fout.write(str(answer))\n",
        "\n",
        "write_answer(mse_root, \"ans2.txt\")"
      ],
      "execution_count":25,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: итого задание 2. RMSE = 5.4552 (ответ анализатора: Правильно. Далее вы увидите, что это на этом датасете это довольно неплохое качество.)."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Задание 3\n",
        "\n",
        "Вас может также беспокоить, что двигаясь с постоянным шагом, вблизи минимума ошибки ответы на обучающей выборке меняются слишком резко, перескакивая через минимум. \n",
        "\n",
        "Попробуйте уменьшать вес перед каждым алгоритмом с каждой следующей итерацией по формуле `0.9 \/ (1.0 + i)`, где `i` - номер итерации (от 0 до 49). Используйте качество работы алгоритма как **ответ в пункте 3**. \n",
        "\n",
        "В реальности часто применяется следующая стратегия выбора шага: как только выбран алгоритм, подберем коэффициент перед ним численным методом оптимизации таким образом, чтобы отклонение от правильных ответов было минимальным. Мы не будем предлагать вам реализовать это для выполнения задания, но рекомендуем попробовать разобраться с такой стратегией и реализовать ее при случае для себя."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: Решения Задания 3. Цель задания - реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь (величина шага уменьшается по формуле)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем число решающих деревьев (базовых алгоритмов)\n",
        "N = 50\n",
        "#vibo: список с базовыми алгоритмами\n",
        "base_algorithms_list = []\n",
        "#vibo: список с коэффициентами перед алгоритмами\n",
        "coefficients_list = []\n",
        "#vibo: шаг алгоритма\n",
        "eta = 0.9\n",
        "#vibo: список ошибок\n",
        "mse_root_list = []"
      ],
      "execution_count":40,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем начальный алгоритм и обучаем его\n",
        "regressor = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "#vibo: обучаем на train\n",
        "regressor.fit(X_train, y_train)\n",
        "#vibo: добавляем первым в список\n",
        "base_algorithms_list.append(regressor)\n",
        "#vibo: добавляем первый коэффициент в список\n",
        "coefficients_list.append(eta)\n",
        "\n",
        "#vibo: обучаем решающие деревья\n",
        "for i in range(N-1):\n",
        "    #vibo: рассчитываем сдвиг на train как истинный ответ минус предсказание\n",
        "    #vibo: при работе функции gmb_predict берем из списка алгоритмов - начальный, считаем прогноз, домножаем на eta\n",
        "    s = y_train - gbm_predict(X_train)\n",
        "    #vibo: задаем алгоритм\n",
        "    regressor = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "    #vibo: обучаем алгоритм, приближаем к s\n",
        "    regressor.fit(X_train, s)\n",
        "    base_algorithms_list.append(regressor)\n",
        "    #vibo: уменьшаем коэффициент на каждом шаге по заданной формуле\n",
        "    eta = 0.9 \/ (1 + (i+1))\n",
        "    coefficients_list.append(eta)\n",
        "    #vibo: вычисляем ошибку на test\n",
        "    mse_root = (mean_squared_error(y_test, gbm_predict(X_test)))**0.5 \n",
        "    mse_root_list.append(mse_root)"
      ],
      "execution_count":41,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mse_root_list"
      ],
      "execution_count":42,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "[4.411594948336074,\n",
              " 4.391183171661546,\n",
              " 4.458775073285299,\n",
              " 4.498661031729782,\n",
              " 4.5168152101676124,\n",
              " 4.550526944390747,\n",
              " 4.609092468715132,\n",
              " 4.6339875745933385,\n",
              " 4.651558505714779,\n",
              " 4.659211687810463,\n",
              " 4.665846192244087,\n",
              " 4.687756339977889,\n",
              " 4.705212386904845,\n",
              " 4.70888638920925,\n",
              " 4.727160820532567,\n",
              " 4.738601041184404,\n",
              " 4.7434294144854805,\n",
              " 4.747957771408691,\n",
              " 4.754535803819599,\n",
              " 4.760702807935853,\n",
              " 4.76469042919606,\n",
              " 4.764828870587975,\n",
              " 4.763080541850576,\n",
              " 4.763091173078234,\n",
              " 4.762268473295419,\n",
              " 4.758055256544697,\n",
              " 4.768499657813316,\n",
              " 4.7718046637409675,\n",
              " 4.780389023483234,\n",
              " 4.7765763172358175,\n",
              " 4.783744654359199,\n",
              " 4.7865280216634964,\n",
              " 4.788442367935862,\n",
              " 4.787212053443569,\n",
              " 4.792422462196972,\n",
              " 4.793468730432272,\n",
              " 4.7994641171508885,\n",
              " 4.804033058481719,\n",
              " 4.8038723728476995,\n",
              " 4.803086837269195,\n",
              " 4.804028978406079,\n",
              " 4.805470455205758,\n",
              " 4.804790977053986,\n",
              " 4.803709717063459,\n",
              " 4.804773448498893,\n",
              " 4.808927271733807,\n",
              " 4.808240835910818,\n",
              " 4.80887898153513,\n",
              " 4.812550945781193]"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def write_answer(answer, filename):\n",
        "    with open(filename, 'w') as fout:\n",
        "        fout.write(str(answer))\n",
        "\n",
        "write_answer(mse_root, \"ans3.txt\")"
      ],
      "execution_count":43,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: итого задание 3. RMSE = 4.8125 (ответ анализатора: Верно. Обратите внимание, что более аккуратный выбор шага позволил понизить RMSE на тестовой выборке (если нет - попробуйте перезапустить алгоритм несколько раз - в среднем качество должно было улучшиться). Однако не стоит относиться к этому результату слишком доверчиво - небольшие изменения в формуле вычисления величины шага могут легко \"сломать\" этот эффект. Выбор хорошего шага в градиентном спуске всегда достаточно непростой вопрос - остается только порадоваться, что почти всегда можно использовать готовые реализации из библиотек.)."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Задание 4\n",
        "\n",
        "Реализованный вами метод - градиентный бустинг над деревьями - очень популярен в машинном обучении. Он представлен как в самой библиотеке `sklearn`, так и в сторонней библиотеке `XGBoost`, которая имеет свой питоновский интерфейс. На практике `XGBoost` работает заметно лучше `GradientBoostingRegressor` из `sklearn`, но для этого задания вы можете использовать любую реализацию. \n",
        "\n",
        "Исследуйте, переобучается ли градиентный бустинг с ростом числа итераций (и подумайте, почему), а также с ростом глубины деревьев. На основе наблюдений выпишите через пробел номера правильных из приведенных ниже утверждений в порядке возрастания номера (это будет **ответ в п.4**):\n",
        "\n",
        "    1. С увеличением числа деревьев, начиная с некоторого момента, качество работы градиентного бустинга не меняется существенно.\n",
        "\n",
        "    2. С увеличением числа деревьев, начиная с некоторого момента, градиентный бустинг начинает переобучаться.\n",
        "\n",
        "    3. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга на тестовой выборке начинает ухудшаться.\n",
        "\n",
        "    4. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга перестает существенно изменяться"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем число решающих деревьев (базовых алгоритмов)\n",
        "N = 50\n",
        "#vibo: список с базовыми алгоритмами\n",
        "base_algorithms_list = []\n",
        "#vibo: список с коэффициентами перед алгоритмами\n",
        "coefficients_list = []\n",
        "#vibo: шаг алгоритма\n",
        "eta = 0.9\n",
        "#vibo: список ошибок\n",
        "mse_root_list = []"
      ],
      "execution_count":51,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#vibo: задаем начальный алгоритм и обучаем его\n",
        "regressor = tree.DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "#vibo: обучаем на train\n",
        "regressor.fit(X_train, y_train)\n",
        "#vibo: добавляем первым в список\n",
        "base_algorithms_list.append(regressor)\n",
        "#vibo: добавляем первый коэффициент в список\n",
        "coefficients_list.append(eta)\n",
        "\n",
        "#vibo: обучаем решающие деревья\n",
        "for i in range(N-1):\n",
        "    #vibo: рассчитываем сдвиг на train как истинный ответ минус предсказание\n",
        "    #vibo: при работе функции gmb_predict берем из списка алгоритмов - начальный, считаем прогноз, домножаем на eta\n",
        "    s = y_train - gbm_predict(X_train)\n",
        "    #vibo: задаем алгоритм\n",
        "    regressor = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "    #vibo: обучаем алгоритм, приближаем к s\n",
        "    regressor.fit(X_train, s)\n",
        "    base_algorithms_list.append(regressor)\n",
        "    coefficients_list.append(eta)\n",
        "    #vibo: вычисляем ошибку на test\n",
        "    mse_root = (mean_squared_error(y_test, gbm_predict(X_test)))**0.5 \n",
        "    mse_root_list.append(mse_root)"
      ],
      "execution_count":52,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mse_root_list"
      ],
      "execution_count":53,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "[5.605855724198541,\n",
              " 5.733275412567501,\n",
              " 5.830622977259917,\n",
              " 5.8149034264110435,\n",
              " 5.804078213149917,\n",
              " 5.819443313672626,\n",
              " 5.862998614211611,\n",
              " 5.804048123104557,\n",
              " 5.813148418304858,\n",
              " 5.791520374141209,\n",
              " 5.804803706791228,\n",
              " 5.817758725610122,\n",
              " 5.798646880976972,\n",
              " 5.796333731150376,\n",
              " 5.799268991848334,\n",
              " 5.798040968456205,\n",
              " 5.800439420320781,\n",
              " 5.8011659667460185,\n",
              " 5.8009768800113655,\n",
              " 5.805025626011457,\n",
              " 5.805758282440396,\n",
              " 5.807062424923722,\n",
              " 5.80811400872747,\n",
              " 5.8079300048299025,\n",
              " 5.809262335951413,\n",
              " 5.805821153036669,\n",
              " 5.804069648260911,\n",
              " 5.804894655970073,\n",
              " 5.806323518336248,\n",
              " 5.806820196236679,\n",
              " 5.806607775272545,\n",
              " 5.806516887427267,\n",
              " 5.806227951011707,\n",
              " 5.805924163417271,\n",
              " 5.805781020112391,\n",
              " 5.805723719764913,\n",
              " 5.805823216256652,\n",
              " 5.805394836340637,\n",
              " 5.805430664696475,\n",
              " 5.805382837028156,\n",
              " 5.8054074868949455,\n",
              " 5.805310057394385,\n",
              " 5.805649724105423,\n",
              " 5.805614713938634,\n",
              " 5.805732516796667,\n",
              " 5.805816955599942,\n",
              " 5.80570927593649,\n",
              " 5.805695568906455,\n",
              " 5.805825928191022]"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: итого задание 4. Верно 2. из RMSE выше видно, что градиентный бустинг переобучается с ростом количества решающих деревьев. Верно 3. RMSE еще сильнее ростет с увеличением глубины решающих деревьев, композиция еще хуже, чем при увеличении числа решающих деревьев. (ответ анализатора: В самом деле, градиентный бустинг все больше подгоняется под данные с ростом числа деревьев, а рост глубины деревьев только ускоряет этот процесс. Начиная с некоторого момента алгоритм будет все больше переобучаться.)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "write_answer('2 3', \"ans4.txt\")"
      ],
      "execution_count":54,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Задание 5\n",
        "\n",
        "Сравните получаемое с помощью градиентного бустинга качество с качеством работы линейной регрессии. \n",
        "\n",
        "Для этого обучите `LinearRegression` из `sklearn.linear_model` (с параметрами по умолчанию) на обучающей выборке и оцените для прогнозов полученного алгоритма на тестовой выборке `RMSE`. Полученное качество - ответ в **пункте 5**. \n",
        "\n",
        "В данном примере качество работы простой модели должно было оказаться хуже, но не стоит забывать, что так бывает не всегда. В заданиях к этому курсу вы еще встретите пример обратной ситуации."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from sklearn import linear_model\n",
        "\n",
        "linear_regressor = linear_model.LinearRegression()\n",
        "linear_regressor.fit(X_train, y_train)\n",
        "mse_root = (mean_squared_error(y_test, linear_regressor.predict(X_test)))**0.5"
      ],
      "execution_count":58,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mse_root"
      ],
      "execution_count":59,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "8.254979753549398"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "write_answer(mse_root, \"ans5.txt\")"
      ],
      "execution_count":60,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### vibo: итого задание 5. mse_root для линейной модели = 8.2550 сильно хуже, чем решающие деревья (ответ анализатора: Качество работы простого метода (линейной регрессии) оказалось хуже. Этот результат в некоторой степени завораживает: всего 1 деревьев, каждое из которых в каждом своем листе оценивает целевую зависимость некоторой константой, уже решили задачу регрессии лучше, чем линейная модель.)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}